2022-12-21 19:40:10,323 fflow.py initialize [line:259] INFO Using Logger in `utils.logger.basic_logger`
2022-12-21 19:40:10,324 fflow.py initialize [line:260] INFO Initializing fedtask: pill_classification
2022-12-21 19:40:10,399 fflow.py initialize [line:276] INFO Using model `resnet18` in `benchmark.pill_classification.model.resnet18` as the globally shared model.
2022-12-21 19:40:10,402 fflow.py initialize [line:304] INFO Using sampler `threshold_sampler` in `sampler.threshold_sampler` as the globally shared model.
Using cache found in /home/aiotlabws/.cache/torch/hub/pytorch_vision_v0.10.0
2022-12-21 19:40:21,777 fflow.py initialize [line:324] INFO Initializing Clients: 10 clients of `algorithm.fedalgo1_fedprox.Client` being created.
2022-12-21 19:40:21,784 fflow.py initialize [line:345] INFO Initializing Server: 1 server of `algorithm.fedalgo1_fedprox.Server` being created.
2022-12-21 19:40:21,786 fflow.py initialize [line:358] INFO Ready to start.
2022-12-21 19:40:21,787 fedbase.py run [line:61] INFO --------------Round 1--------------
2022-12-21 19:40:45,510 main.py main [line:27] ERROR Exception Logged
Traceback (most recent call last):
  File "main.py", line 24, in main
    server.run()
  File "/mnt/disk2/ndlong/easyFL/algorithm/fedbase.py", line 65, in run
    flw.logger.log_once()
  File "/mnt/disk2/ndlong/easyFL/utils/logger/basic_logger.py", line 202, in log_once
    test_metric = self.server.test()
  File "/mnt/disk2/ndlong/easyFL/algorithm/fedbase.py", line 297, in test
    return self.calculator.test(
  File "/mnt/disk1/anaconda3/envs/longnd/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/mnt/disk2/ndlong/easyFL/benchmark/toolkits.py", line 692, in test
    outputs = model(batch_data[0])
  File "/mnt/disk1/anaconda3/envs/longnd/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/disk2/ndlong/easyFL/benchmark/pill_classification/model/resnet18.py", line 159, in forward
    x = self.model(x)
  File "/mnt/disk1/anaconda3/envs/longnd/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/disk1/anaconda3/envs/longnd/lib/python3.8/site-packages/torchvision/models/resnet.py", line 249, in forward
    return self._forward_impl(x)
  File "/mnt/disk1/anaconda3/envs/longnd/lib/python3.8/site-packages/torchvision/models/resnet.py", line 233, in _forward_impl
    x = self.bn1(x)
  File "/mnt/disk1/anaconda3/envs/longnd/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/disk1/anaconda3/envs/longnd/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py", line 168, in forward
    return F.batch_norm(
  File "/mnt/disk1/anaconda3/envs/longnd/lib/python3.8/site-packages/torch/nn/functional.py", line 2282, in batch_norm
    return torch.batch_norm(
RuntimeError: CUDA out of memory. Tried to allocate 1.53 GiB (GPU 0; 23.70 GiB total capacity; 1.86 GiB already allocated; 658.81 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "main.py", line 24, in main
    server.run()
  File "/mnt/disk2/ndlong/easyFL/algorithm/fedbase.py", line 65, in run
    flw.logger.log_once()
  File "/mnt/disk2/ndlong/easyFL/utils/logger/basic_logger.py", line 202, in log_once
    test_metric = self.server.test()
  File "/mnt/disk2/ndlong/easyFL/algorithm/fedbase.py", line 297, in test
    return self.calculator.test(
  File "/mnt/disk1/anaconda3/envs/longnd/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/mnt/disk2/ndlong/easyFL/benchmark/toolkits.py", line 692, in test
    outputs = model(batch_data[0])
  File "/mnt/disk1/anaconda3/envs/longnd/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/disk2/ndlong/easyFL/benchmark/pill_classification/model/resnet18.py", line 159, in forward
    x = self.model(x)
  File "/mnt/disk1/anaconda3/envs/longnd/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/disk1/anaconda3/envs/longnd/lib/python3.8/site-packages/torchvision/models/resnet.py", line 249, in forward
    return self._forward_impl(x)
  File "/mnt/disk1/anaconda3/envs/longnd/lib/python3.8/site-packages/torchvision/models/resnet.py", line 233, in _forward_impl
    x = self.bn1(x)
  File "/mnt/disk1/anaconda3/envs/longnd/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/disk1/anaconda3/envs/longnd/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py", line 168, in forward
    return F.batch_norm(
  File "/mnt/disk1/anaconda3/envs/longnd/lib/python3.8/site-packages/torch/nn/functional.py", line 2282, in batch_norm
    return torch.batch_norm(
RuntimeError: CUDA out of memory. Tried to allocate 1.53 GiB (GPU 0; 23.70 GiB total capacity; 1.86 GiB already allocated; 658.81 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File "main.py", line 33, in <module>
    main()
  File "main.py", line 28, in main
    raise RuntimeError
RuntimeError